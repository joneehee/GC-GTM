{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM interpretation\n",
        "- Using GPT, Claude, Llama model\n",
        "- Representative model : GPT o4-mini model (API)"
      ],
      "metadata": {
        "id": "maXQANOp3WuB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RtTpDJ13LFv"
      },
      "outputs": [],
      "source": [
        "df_q1_clean_idx = pd.read_csv('q1q4_keywords_idx.csv')\n",
        "\n",
        "df_q1_clean_idx.index = ['V2', 'V3', 'V6', 'V7', 'V12', 'V13', 'V18']\n",
        "df_q1_clean_idx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install uv langchain langchain_openai python-dotenv ipykernel"
      ],
      "metadata": {
        "id": "wccETw2_32g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = #private key\n",
        "\n",
        "# PythonÏóêÏÑú Î°úÎìú\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = #private key\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "def generate_response(system_message, user_message):\n",
        "\n",
        "    # system + user message -> input\n",
        "    prompt = f\"\"\"\n",
        "    <system>\n",
        "    {system_message}\n",
        "    </system>\n",
        "\n",
        "    <user>\n",
        "    {user_message}\n",
        "    </user>\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.responses.create(\n",
        "        model=\"o4-mini\",\n",
        "        input=prompt\n",
        "    )\n",
        "\n",
        "    return response.output_text"
      ],
      "metadata": {
        "id": "pfASPfT036a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_tech_abstract(space_id, filtered_df, top_n=30):\n",
        "    # space_id\n",
        "    if isinstance(space_id, int):\n",
        "        space_id_str = f\"V{space_id}\"\n",
        "    else:\n",
        "        space_id_str = str(space_id)\n",
        "\n",
        "    if space_id_str not in filtered_df.index:\n",
        "        raise ValueError(\n",
        "            f\"space_id '{space_id_str}' not found in filtered_df. \"\n",
        "            f\"Available: {list(filtered_df.index)[:10]}...\"\n",
        "        )\n",
        "\n",
        "    # tech column parsing\n",
        "    tech_str = filtered_df.loc[space_id_str, 'tech']\n",
        "    matches = re.findall(r'([^\\s]+)\\s+\\(([\\d\\.]+)\\)', tech_str)\n",
        "    tech_keywords = {('tech', kw): float(w) for kw, w in matches}\n",
        "\n",
        "    top_keywords = sorted(tech_keywords.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "    weighted_keywords_with_origin = [\n",
        "        f\"{keyword} ({weight:.3f}, origin: {origin})\"\n",
        "        for (origin, keyword), weight in top_keywords\n",
        "    ]\n",
        "    formatted_keywords = \", \".join(weighted_keywords_with_origin)\n",
        "\n",
        "    # ----------------------------\n",
        "    # prompt\n",
        "    # ----------------------------\n",
        "    system_message = \"\"\"\n",
        "      You are an expert in on-device AI and embedded machine learning systems.\n",
        "      Your job is to generate highly specific, technically coherent abstracts strictly grounded in the provided weighted keywords.\n",
        "      The abstract must remain within the semantic scope suggested by the keywords.\n",
        "      \"\"\"\n",
        "\n",
        "    user_message = f\"\"\"\n",
        "      [Weighted keywords]\n",
        "      {formatted_keywords}\n",
        "\n",
        "      [Purpose]\n",
        "      The weighted keywords represent a potential unexplored area in the on-device AI technology domain.\n",
        "      Your objective is to interpret the weighted keywords and propose a novel, coherent technical mechanism aligned with their collective meaning.\n",
        "\n",
        "      [Generation rules]\n",
        "      1. Remain strictly within the **single technical domain** inferred from the highest-weight keywords\n",
        "        across the keyword's origin.\n",
        "        - You are NOT required to use all keywords.\n",
        "        - Lower-weight keywords may be ignored unless they naturally reinforce the dominant concept.\n",
        "      2. Identify **one unifying dominant concept** that both domains can converge on.\n",
        "        - This concept must be the structural center of the entire abstract.\n",
        "        - All mechanisms must be derived from, and consistent with, this dominant concept.\n",
        "      3. Describe a coherent technical mechanism that explains:\n",
        "        - the problem implied jointly,\n",
        "        - the on-device AI method or architecture addressing it,\n",
        "        - and how that origin(tech) contributes functionally to different stages of the mechanism.\n",
        "      4. The abstract must describe a **plausible and detailed on-device architecture**, including: modules, pipelines, computational steps and data flow,\n",
        "      5. DO NOT introduce any fields or topics outside the unified domain.\n",
        "        - No extra modalities, no unrelated algorithms, no foreign technologies.\n",
        "      6. Length: ~500‚Äì600 characters. Style: patent-like, mechanism-focused, concise.\n",
        "      7. Do NOT call the idea ‚Äúnovel.‚Äù\n",
        "        - Novelty must emerge implicitly from the mechanism logically derived from the keyword distribution.\n",
        "\n",
        "      [Output format]\n",
        "      *abstract: [Write the abstract here.]\n",
        "      *reason: [Explain in 2‚Äì3 sentences how the dominant keywords shaped the technical mechanism.]\n",
        "      \"\"\"\n",
        "\n",
        "    print(\"ü§ñ  generate_response start\")\n",
        "    response = generate_response(system_message, user_message)\n",
        "    print(\"üëÄ  generate_response result:\", repr(response))\n",
        "    return response"
      ],
      "metadata": {
        "id": "redkxgm94l4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_both_abstract(space_id, filtered_df, top_n=20):\n",
        "    # space_id\n",
        "    if isinstance(space_id, int):\n",
        "        space_id_str = f\"V{space_id}\"\n",
        "    else:\n",
        "        space_id_str = str(space_id)\n",
        "\n",
        "    if space_id_str not in filtered_df.index:\n",
        "        raise ValueError(\n",
        "            f\"space_id '{space_id_str}' not found in filtered_df. \"\n",
        "            f\"Available: {list(filtered_df.index)[:10]}...\"\n",
        "        )\n",
        "\n",
        "    overlap_str = filtered_df.loc[space_id_str, 'overlapping']\n",
        "    tech_str    = filtered_df.loc[space_id_str, 'tech']\n",
        "\n",
        "    overlap_str = \"\" if pd.isna(overlap_str) else overlap_str\n",
        "    tech_str    = \"\" if pd.isna(tech_str) else tech_str\n",
        "\n",
        "    overlap_matches = re.findall(r'([^\\s]+)\\s+\\(([\\d\\.]+)\\)', overlap_str)\n",
        "    tech_matches    = re.findall(r'([^\\s]+)\\s+\\(([\\d\\.]+)\\)', tech_str)\n",
        "\n",
        "    both_keywords = {}\n",
        "    for kw, w in overlap_matches:\n",
        "        both_keywords[('overlapping', kw)] = float(w)\n",
        "    for kw, w in tech_matches:\n",
        "        both_keywords[('tech', kw)] = float(w)\n",
        "\n",
        "    top_keywords = sorted(both_keywords.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "    weighted_keywords_with_origin = [\n",
        "        f\"{keyword} ({weight:.3f}, origin: {origin})\"\n",
        "        for (origin, keyword), weight in top_keywords\n",
        "    ]\n",
        "    formatted_keywords = \", \".join(weighted_keywords_with_origin)\n",
        "\n",
        "    # ----------------------------\n",
        "    # prompt\n",
        "    # ----------------------------\n",
        "    system_message = \"\"\"\n",
        "      You are an expert in on-device AI and embedded machine learning systems.\n",
        "      Your job is to generate highly specific, technically coherent abstracts strictly grounded in the provided weighted keywords.\n",
        "      The abstract must remain within the semantic scope suggested by the keywords.\n",
        "      \"\"\"\n",
        "\n",
        "    user_message = f\"\"\"\n",
        "      [Weighted keywords]\n",
        "      {formatted_keywords}\n",
        "\n",
        "      [Purpose]\n",
        "      You are given two domains of keywords: 'tech' and 'overlapping'.\n",
        "      They jointly suggest a potential unexplored direction in on-device AI.\n",
        "      Your objective is to integrate both domains into a single, novel, coherent technical mechanism.\n",
        "\n",
        "      [Generation rules]\n",
        "      1. Remain strictly within the **single technical domain** inferred from the highest-weight keywords\n",
        "        across both origins (tech + overlapping).\n",
        "        - You are NOT required to use all keywords.\n",
        "        - Lower-weight keywords may be ignored unless they naturally reinforce the dominant concept.\n",
        "      2. Identify **one unifying dominant concept** that both domains can converge on.\n",
        "        - This concept must be the structural center of the entire abstract.\n",
        "        - All mechanisms must be derived from, and consistent with, this dominant concept.\n",
        "      3. Describe a coherent technical mechanism that explains:\n",
        "        - the problem implied jointly by the tech and overlapping signals,\n",
        "        - the on-device AI method or architecture addressing it,\n",
        "        - and how each origin (tech / overlapping) contributes functionally\n",
        "          to different stages of the mechanism.\n",
        "      4. The abstract must describe a **plausible and detailed on-device architecture**, including:\n",
        "        - modules, pipelines, or hardware‚Äìfirmware interaction,\n",
        "        - computational steps and data flow,\n",
        "        - without diverging from the dominant concept.\n",
        "      5. DO NOT introduce any fields or topics outside the unified domain.\n",
        "        - No extra modalities, no unrelated algorithms, no foreign technologies.\n",
        "      6. Length: ~500‚Äì600 characters. Style: patent-like, mechanism-focused, concise.\n",
        "      7. Do NOT call the idea ‚Äúnovel.‚Äù\n",
        "        - Novelty must emerge implicitly from the mechanism logically derived from the keyword distribution.\n",
        "\n",
        "      [Output format]\n",
        "      *abstract: [Write the abstract here.]\n",
        "      *reason: [Explain in 2‚Äì3 sentences how you combined 'tech' and 'overlapping' signals into one coherent mechanism.]\n",
        "      \"\"\"\n",
        "\n",
        "    print(\"ü§ñ  generate_response start\")\n",
        "    response = generate_response(system_message, user_message)\n",
        "    print(\"üëÄ  generate_response result:\", repr(response))\n",
        "    return response"
      ],
      "metadata": {
        "id": "2hT8ITVV4q5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "target_space_ids = [f'V{i}' for i in [2, 3, 6, 7, 12, 13, 18]]\n",
        "\n",
        "#tech(baseline) interpretation\n",
        "results_df = pd.DataFrame(columns=[\"space_id\", \"abstract(tech)\"])\n",
        "\n",
        "import time\n",
        "\n",
        "for space_id in target_space_ids:\n",
        "    print(f\"\\n===================== {space_id} =====================\")\n",
        "    try:\n",
        "        abstract = generate_tech_abstract(space_id, df_q1_clean_idx)\n",
        "        new_row = pd.DataFrame({\n",
        "            'space_id': [space_id],\n",
        "            'abstract(tech)': [abstract]\n",
        "        })\n",
        "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
        "        print(f\"Completed abstract for space ID {space_id}\")\n",
        "        time.sleep(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error for space ID {space_id} ‚Üí {e}\")\n",
        "        continue\n",
        "\n",
        "\n",
        "# BOTH(ours) interpretation\n",
        "if 'abstract(both)' not in results_df.columns:\n",
        "    results_df['abstract(both)'] = None\n",
        "\n",
        "for space_id in target_space_ids:\n",
        "    print(f\"\\n[BOTH] ===================== {space_id} =====================\")\n",
        "    try:\n",
        "        abstract_both = generate_both_abstract(space_id, df_q1_clean_idx)\n",
        "        results_df.loc[results_df['space_id'] == space_id, 'abstract(both)'] = abstract_both\n",
        "        print(f\"Completed BOTH abstract for space ID {space_id}\")\n",
        "        time.sleep(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error (BOTH) for space ID {space_id} ‚Üí {e}\")\n",
        "        continue\n",
        "\n",
        "results_df.to_csv(\"abstract_gpt_5.csv\", index=False)\n",
        "print(\"\\n‚≠ê Clear! abstract_gpt_5.csv\")"
      ],
      "metadata": {
        "id": "G_Xtlhn75Qg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "dK612h375soI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding (generated text)\n",
        "- GPT 5 sets\n",
        "- Claude 5 sets\n",
        "- Llama 5 sets"
      ],
      "metadata": {
        "id": "Gggc2btS6qou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "lgTdlhQd5Zsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#embedding\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "save_path = \"/content/LLMtext\"\n",
        "model = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')\n",
        "\n",
        "abs_gpt_1 = pd.read_csv(f\"{save_path}/abstract_gpt_1.csv\", encoding='cp949')\n",
        "abs_gpt_2 = pd.read_csv(f\"{save_path}/abstract_gpt_2.csv\", encoding='cp949')\n",
        "abs_gpt_3 = pd.read_csv(f\"{save_path}/abstract_gpt_3.csv\", encoding='cp949')\n",
        "abs_gpt_4 = pd.read_csv(f\"{save_path}/abstract_gpt_4.csv\", encoding='cp949')\n",
        "abs_gpt_5 = pd.read_csv(f\"{save_path}/abstract_gpt_5.csv\", encoding='cp949')\n",
        "\n",
        "abs_claude_1 = pd.read_csv(f\"{save_path}/abstract_claude_1.csv\", encoding='cp949')\n",
        "abs_claude_2 = pd.read_csv(f\"{save_path}/abstract_claude_2.csv\", encoding='cp949')\n",
        "abs_claude_3 = pd.read_csv(f\"{save_path}/abstract_claude_3.csv\", encoding='cp949')\n",
        "abs_claude_4 = pd.read_csv(f\"{save_path}/abstract_claude_4.csv\", encoding='cp949')\n",
        "abs_claude_5 = pd.read_csv(f\"{save_path}/abstract_claude_5.csv\", encoding='cp949')\n",
        "\n",
        "abs_llama_1 = pd.read_csv(f\"{save_path}/abstract_llama_1.csv\", encoding='cp949')\n",
        "abs_llama_2 = pd.read_csv(f\"{save_path}/abstract_llama_2.csv\", encoding='cp949')\n",
        "abs_llama_3 = pd.read_csv(f\"{save_path}/abstract_llama_3.csv\", encoding='cp949')\n",
        "abs_llama_4 = pd.read_csv(f\"{save_path}/abstract_llama_4.csv\", encoding='cp949')\n",
        "abs_llama_5 = pd.read_csv(f\"{save_path}/abstract_llama_5.csv\", encoding='cp949')"
      ],
      "metadata": {
        "id": "NP_dPDrS52YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/Colab Notebooks/LLMtext\"\n",
        "\n",
        "# embedding model\n",
        "model = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')\n",
        "\n",
        "# text columns name\n",
        "base_text_cols = [\n",
        "    'gpt(tech)',\n",
        "    'gpt(both)',\n",
        "    'claude(tech)',\n",
        "    'claude(both)',\n",
        "    'llama(tech)',\n",
        "    'llama(both)',\n",
        "]\n",
        "\n",
        "# 1~5 repeat\n",
        "for i in range(1, 6):\n",
        "    print(f\"\\n===== {i}ÌöåÏ∞® Ï≤òÎ¶¨ ÏãúÏûë =====\")\n",
        "\n",
        "    dfs = []\n",
        "\n",
        "    # get gpt_i, claude_i, llama_i\n",
        "    for model_name in ['gpt', 'claude', 'llama']:\n",
        "        var_name = f'abs_{model_name}_{i}'\n",
        "        if var_name in globals():\n",
        "            print(f\" - {var_name} ÏÇ¨Ïö©\")\n",
        "            dfs.append(globals()[var_name])\n",
        "        else:\n",
        "            print(f\" - {var_name} ÏóÜÏùå (Ïä§ÌÇµ)\")\n",
        "\n",
        "    if not dfs:\n",
        "        print(f\"{i}ÌöåÏ∞®: ÏÇ¨Ïö©Ìï† DataFrameÏù¥ ÏóÜÏñ¥ Ïä§ÌÇµÌï©ÎãàÎã§.\")\n",
        "        continue\n",
        "\n",
        "    val = pd.concat(dfs, axis=1)\n",
        "    val = val.loc[:, ~val.columns.duplicated()]\n",
        "\n",
        "    text_cols = [col for col in base_text_cols if col in val.columns]\n",
        "\n",
        "    print(f\"{i}: emb col -> {text_cols}\")\n",
        "\n",
        "    # embedding by columns\n",
        "    for col in text_cols:\n",
        "        emb_col = col.replace('(', '_').replace(')', '').replace(' ', '_') + '_emb'\n",
        "        print(f\"Encoding column: {col} -> {emb_col}\")\n",
        "\n",
        "        texts = val[col].fillna(\"\").astype(str).tolist()\n",
        "        embeddings = model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "        val[emb_col] = embeddings.tolist()\n",
        "\n",
        "    pkl_path = f\"{save_path}/abstract_all_{i}_with_embeddings.pkl\"\n",
        "    val.to_pickle(pkl_path)\n",
        "\n",
        "    print(f\"{i}: save -> {pkl_path}\")"
      ],
      "metadata": {
        "id": "GfFqwp3a6AC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding (train data)"
      ],
      "metadata": {
        "id": "A4EDJ65k6zBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')\n",
        "\n",
        "train2 = pd.read_csv(f\"{save_path}/train_data.csv\")\n",
        "train_texts = train2['text'].tolist()\n",
        "\n",
        "train_embeddings = model.encode(train_texts, batch_size=16, show_progress_bar=True)\n",
        "print(\"train_embeddings.shape:\", train_embeddings.shape)\n",
        "\n",
        "train2['embedding'] = train_embeddings.tolist()\n",
        "train2.to_csv(f\"{save_path}/train_emb2.csv\", index=False)"
      ],
      "metadata": {
        "id": "sMy4TkNS62IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Novelty (LOF score)"
      ],
      "metadata": {
        "id": "BkonRVTQ7Ds8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "import ast\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ======================================================\n",
        "# 1. Train embedding\n",
        "# ======================================================\n",
        "train_emb = pd.read_csv(f\"{save_path}/train_emb2.csv\")\n",
        "\n",
        "train_emb[\"embedding\"] = train_emb[\"embedding\"].apply(ast.literal_eval)\n",
        "\n",
        "# (n_train, dim) array\n",
        "train_embeddings_arr = np.vstack(train_emb[\"embedding\"].values)\n",
        "\n",
        "# ======================================================\n",
        "# 2. LOF train\n",
        "# ======================================================\n",
        "lof = LocalOutlierFactor(\n",
        "    n_neighbors=20,\n",
        "    metric='cosine',\n",
        "    novelty=True\n",
        ")\n",
        "lof.fit(train_embeddings_arr)\n",
        "\n",
        "# ======================================================\n",
        "# 3. LOF score\n",
        "# ======================================================\n",
        "target_columns = {\n",
        "    'gpt_tech_emb'    : 'gpt_tech_lof',\n",
        "    'gpt_both_emb'    : 'gpt_both_lof',\n",
        "    'claude_tech_emb' : 'claude_tech_lof',\n",
        "    'claude_both_emb' : 'claude_both_lof',\n",
        "    'llama_tech_emb'  : 'llama_tech_lof',\n",
        "    'llama_both_emb'  : 'llama_both_lof'\n",
        "}\n",
        "\n",
        "for i in range(1, 6):\n",
        "    print(f\"\\n===== {i} LOF start =====\")\n",
        "\n",
        "    pkl_path = f\"{save_path}/abstract_all_{i}_with_embeddings.pkl\"\n",
        "\n",
        "    try:\n",
        "        val = pd.read_pickle(pkl_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\" - {pkl_path} none ‚Üí skip\\n\")\n",
        "        continue\n",
        "\n",
        "    existing_emb_cols = [col for col in target_columns.keys() if col in val.columns]\n",
        "    print(f\" - emb col: {existing_emb_cols}\")\n",
        "\n",
        "    for emb_col in existing_emb_cols:\n",
        "        score_col = target_columns[emb_col]\n",
        "        print(f\"   LOF scoring: {emb_col} -> {score_col}\")\n",
        "\n",
        "        target_embeddings = np.array(val[emb_col].tolist())\n",
        "\n",
        "        # LOF score\n",
        "        lof_scores = lof.decision_function(target_embeddings)\n",
        "        val[score_col] = np.round(lof_scores, 4)\n",
        "\n",
        "    novelty_csv = f\"{save_path}/val_novelty_{i}.csv\"\n",
        "    score_cols = [target_columns[c] for c in existing_emb_cols]\n",
        "\n",
        "    val_subset = val[['space_id'] + score_cols].copy()\n",
        "    val_subset = val_subset.rename(columns={'space_id': 'vacancy_num'})\n",
        "\n",
        "    val_subset.to_csv(novelty_csv, index=False, encoding='utf-8-sig')\n",
        "    print(f\" - save ‚Üí {novelty_csv}\")\n"
      ],
      "metadata": {
        "id": "vuI460HL7GLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "novelty1 = pd.read_csv(f\"{save_path}/val_novelty_1.csv\")\n",
        "novelty2 = pd.read_csv(f\"{save_path}/val_novelty_2.csv\")\n",
        "novelty3 = pd.read_csv(f\"{save_path}/val_novelty_3.csv\")\n",
        "novelty4 = pd.read_csv(f\"{save_path}/val_novelty_4.csv\")\n",
        "novelty5 = pd.read_csv(f\"{save_path}/val_novelty_5.csv\")"
      ],
      "metadata": {
        "id": "Ci3S73JB7GOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feasibility (ours/baseline)\n",
        "\n"
      ],
      "metadata": {
        "id": "TUgLxvxG7fqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "test = pd.read_csv(f\"{save_path}/test_data.csv\")\n",
        "\n",
        "test_texts = test['text'].tolist()\n",
        "\n",
        "model = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')\n",
        "\n",
        "test_embeddings = model.encode(test_texts, show_progress_bar=True)\n",
        "test['embedding'] = test_embeddings.tolist()\n",
        "\n",
        "test.to_csv(f\"{save_path}/test_emb.csv\", index=False)"
      ],
      "metadata": {
        "id": "32kRIrkQ7GTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/Colab Notebooks/LLMtext\"\n",
        "print()\n",
        "# ---------------------------\n",
        "# 1. train / test embedding load\n",
        "# ---------------------------\n",
        "test = pd.read_csv(f\"{save_path}/test_emb.csv\")\n",
        "train_df = pd.read_csv(f\"{save_path}/train_emb2.csv\")\n",
        "\n",
        "if isinstance(train_df['embedding'].iloc[0], str):\n",
        "    train_df['embedding'] = train_df['embedding'].apply(ast.literal_eval)\n",
        "\n",
        "# train embeddings (list of list ‚Üí array)\n",
        "train_embeddings = train_df['embedding'].to_numpy()\n",
        "if isinstance(train_embeddings[0], str):\n",
        "    train_embeddings = [ast.literal_eval(e) for e in train_embeddings]\n",
        "train_matrix = np.vstack(train_embeddings)   # (n_train, dim)\n",
        "\n",
        "if isinstance(test['embedding'].iloc[0], str):\n",
        "    test['embedding'] = test['embedding'].apply(ast.literal_eval)\n",
        "test_matrix  = np.vstack(test['embedding'].to_numpy())  # (n_test, dim)\n",
        "\n",
        "train_test_sim = cosine_similarity(train_matrix, test_matrix)  # (n_train, n_test)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "model_emb_cols = {\n",
        "    \"gpt\": \"gpt_both_emb\",\n",
        "    \"claude\": \"claude_both_emb\",\n",
        "    \"llama\": \"llama_both_emb\",\n",
        "}\n",
        "\n",
        "top_k = 10\n",
        "\n",
        "# ---------------------------\n",
        "#  1~5 repeat\n",
        "# ---------------------------\n",
        "for i in range(1, 6):\n",
        "    print(f\"\\n===== {i}ÌöåÏ∞® Feasibility F-ratio (Top-{top_k}) start =====\")\n",
        "\n",
        "    pkl_path = f\"{save_path}/abstract_all_{i}_with_embeddings.pkl\"\n",
        "\n",
        "    try:\n",
        "        val = pd.read_pickle(pkl_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\" - {pkl_path} none ‚Üí skip\")\n",
        "        continue\n",
        "\n",
        "    feas_table = pd.DataFrame({'vacancy_num': val['space_id']})\n",
        "\n",
        "    for model_name, emb_col in model_emb_cols.items():\n",
        "        if emb_col not in val.columns:\n",
        "            print(f\" - {emb_col} none ‚Üí {model_name} skip\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing model: {model_name} (col: {emb_col})\")\n",
        "\n",
        "        if isinstance(val[emb_col].iloc[0], str):\n",
        "            val[emb_col] = val[emb_col].apply(ast.literal_eval)\n",
        "\n",
        "        # (n_val, embed_dim)\n",
        "        val_matrix = np.vstack(val[emb_col].to_numpy())\n",
        "\n",
        "        # 1) generated ‚Üí train similarity : find ancestor train(top-1)\n",
        "        sim_val_to_train = cosine_similarity(val_matrix, train_matrix)  # (n_val, n_train)\n",
        "        nearest_train_idx = sim_val_to_train.argmax(axis=1)             # (n_val,)\n",
        "\n",
        "        # 2) baseline(model@): train ‚Üí test similarity top-k\n",
        "        baseline_mat = train_test_sim[nearest_train_idx]                # (n_val, n_test)\n",
        "        baseline_sorted = np.sort(baseline_mat, axis=1)[:, ::-1]\n",
        "        baseline_topk = baseline_sorted[:, :top_k]                      # (n_val, top_k)\n",
        "        baseline_topk_avg = baseline_topk.mean(axis=1)                  # (n_val,)\n",
        "\n",
        "        # 3) ours(model@): generated ‚Üí test similarity top-k\n",
        "        sim_val_to_test = cosine_similarity(val_matrix, test_matrix)    # (n_val, n_test)\n",
        "        ours_sorted = np.sort(sim_val_to_test, axis=1)[:, ::-1]\n",
        "        ours_topk = ours_sorted[:, :top_k]\n",
        "        ours_topk_avg = ours_topk.mean(axis=1)\n",
        "\n",
        "        # 4) F-ratio = ours / baseline\n",
        "        F_ratio = ours_topk_avg / (baseline_topk_avg + 1e-8)\n",
        "\n",
        "        baseline_col_name = f'{model_name}_baseline'\n",
        "        ours_col_name     = f'{model_name}_ours'\n",
        "        ratio_col_name    = f'{model_name}_F_ratio'\n",
        "\n",
        "        feas_table[baseline_col_name] = baseline_topk_avg\n",
        "        feas_table[ours_col_name]     = ours_topk_avg\n",
        "        feas_table[ratio_col_name]    = F_ratio\n",
        "\n",
        "    out_csv = f\"{save_path}/feasibility_Fratio_top{top_k}_{i}.csv\"\n",
        "    feas_table.to_csv(out_csv, index=False, encoding='utf-8-sig')\n",
        "    print(f\" - save ‚Üí {out_csv}\")"
      ],
      "metadata": {
        "id": "f_QWWD5D7ngW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feasibility1 = pd.read_csv(f\"{save_path}/feasibility_Fratio_top10_1.csv\")\n",
        "feasibility2 = pd.read_csv(f\"{save_path}/feasibility_Fratio_top10_2.csv\")\n",
        "feasibility3 = pd.read_csv(f\"{save_path}/feasibility_Fratio_top10_3.csv\")\n",
        "feasibility4 = pd.read_csv(f\"{save_path}/feasibility_Fratio_top10_4.csv\")\n",
        "feasibility5 = pd.read_csv(f\"{save_path}/feasibility_Fratio_top10_5.csv\")"
      ],
      "metadata": {
        "id": "iDUeKAGp8j7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trend-fit"
      ],
      "metadata": {
        "id": "NrrYuI4S8q-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# -----------------------------------------\n",
        "# emb columns\n",
        "# -----------------------------------------\n",
        "embedding_cols = [\n",
        "    \"gpt_tech_emb\", \"gpt_both_emb\",\n",
        "    \"claude_tech_emb\", \"claude_both_emb\",\n",
        "    \"llama_tech_emb\", \"llama_both_emb\"\n",
        "]\n",
        "\n",
        "# -----------------------------------------\n",
        "# 1~5 repeat\n",
        "# -----------------------------------------\n",
        "for i in range(1, 6):\n",
        "    print(f\"\\n===== {i} Trend-Fit start =====\")\n",
        "\n",
        "    pkl_path = f\"{save_path}/abstract_all_{i}_with_embeddings.pkl\"\n",
        "\n",
        "    try:\n",
        "        val = pd.read_pickle(pkl_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\" - {pkl_path} none ‚Üí skip\")\n",
        "        continue\n",
        "\n",
        "    existing_cols = [col for col in embedding_cols if col in val.columns]\n",
        "    print(f\" - emb col: {existing_cols}\")\n",
        "\n",
        "    trend_table = pd.DataFrame({\"vacancy_num\": val[\"space_id\"]})\n",
        "\n",
        "    # -----------------------------------------\n",
        "    #  trend-fit score\n",
        "    # -----------------------------------------\n",
        "    for col in existing_cols:\n",
        "        print(f\"Processing (trend-fit): {col}\")\n",
        "\n",
        "        if isinstance(val[col].iloc[0], str):\n",
        "            val[col] = val[col].apply(ast.literal_eval)\n",
        "\n",
        "        # (val_len, dim)\n",
        "        val_matrix = np.vstack(val[col].to_numpy())\n",
        "\n",
        "        # cosine similarity\n",
        "        sim_to_test  = cosine_similarity(val_matrix, test_matrix)\n",
        "        sim_to_train = cosine_similarity(val_matrix, train_matrix)\n",
        "\n",
        "        avg_sim_test  = sim_to_test.mean(axis=1)\n",
        "        avg_sim_train = sim_to_train.mean(axis=1)\n",
        "\n",
        "        # Trend-Fit Ratio\n",
        "        trend = avg_sim_test / (avg_sim_train + 1e-8)\n",
        "\n",
        "        base = col.replace(\"_emb\", \"\")\n",
        "        trend_table[f\"{base}_trend\"] = trend.round(4)\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # save\n",
        "    # -----------------------------------------\n",
        "    out_csv = f\"{save_path}/trendfit_{i}.csv\"\n",
        "    trend_table.to_csv(out_csv, index=False, encoding='utf-8-sig')\n",
        "    print(f\" - save ‚Üí {out_csv}\")\n"
      ],
      "metadata": {
        "id": "G-tbFb7s8p9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trendfit1 = pd.read_csv(f\"{save_path}/trendfit_1.csv\")\n",
        "trendfit2 = pd.read_csv(f\"{save_path}/trendfit_2.csv\")\n",
        "trendfit3 = pd.read_csv(f\"{save_path}/trendfit_3.csv\")\n",
        "trendfit4 = pd.read_csv(f\"{save_path}/trendfit_4.csv\")\n",
        "trendfit5 = pd.read_csv(f\"{save_path}/trendfit_5.csv\")"
      ],
      "metadata": {
        "id": "kTyIile_83T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def summarize_runs_by_vacancy(dfs, index_col='vacancy_num', float_decimals=3):\n",
        "    \"\"\"\n",
        "    -> DataFrame ('mean¬±std')\n",
        "    \"\"\"\n",
        "\n",
        "    aligned = []\n",
        "    for df in dfs:\n",
        "        df2 = df.copy()\n",
        "        df2 = df2.set_index(index_col).sort_index()\n",
        "        aligned.append(df2)\n",
        "\n",
        "    base_index = aligned[0].index\n",
        "    base_cols = aligned[0].columns\n",
        "    for k, df in enumerate(aligned[1:], start=2):\n",
        "\n",
        "    # (n_vacancy, n_cols, n_runs)\n",
        "    stack = np.stack([df.values for df in aligned], axis=2)\n",
        "    means = stack.mean(axis=2)\n",
        "    stds  = stack.std(axis=2, ddof=1)\n",
        "\n",
        "    #  mean¬±std\n",
        "    def fmt(m, s):\n",
        "        return f\"{m:.{float_decimals}f}¬±{s:.{float_decimals}f}\"\n",
        "\n",
        "    summary = pd.DataFrame(index=base_index)\n",
        "    for j, col in enumerate(base_cols):\n",
        "        col_means = means[:, j]\n",
        "        col_stds  = stds[:, j]\n",
        "        summary[col] = [fmt(m, s) for m, s in zip(col_means, col_stds)]\n",
        "\n",
        "    # vacancy_num -> columns\n",
        "    summary = summary.reset_index()\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "KMTU_Yj58qDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nov1 = pd.read_csv(f\"{save_path}/val_novelty_1.csv\")\n",
        "nov2 = pd.read_csv(f\"{save_path}/val_novelty_2.csv\")\n",
        "nov3 = pd.read_csv(f\"{save_path}/val_novelty_3.csv\")\n",
        "nov4 = pd.read_csv(f\"{save_path}/val_novelty_4.csv\")\n",
        "nov5 = pd.read_csv(f\"{save_path}/val_novelty_5.csv\")\n",
        "\n",
        "nov_list = [nov1, nov2, nov3, nov4, nov5]\n",
        "\n",
        "nov_summary = summarize_runs_by_vacancy(nov_list, index_col='vacancy_num', float_decimals=3)\n",
        "nov_summary[\"vacancy_num_int\"] = nov_summary[\"vacancy_num\"].str.extract(r\"(\\d+)\").astype(int)\n",
        "nov_summary = nov_summary.sort_values(by=\"vacancy_num_int\").drop(columns=[\"vacancy_num_int\"])\n",
        "\n",
        "# Ï†ÄÏû•\n",
        "display(nov_summary)"
      ],
      "metadata": {
        "id": "J8UVP2uc9LCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feas1 = pd.read_csv(f\"{save_path}/feasibility_Fratio_top10_1.csv\")\n",
        "feas2 = pd.read_csv(f\"{save_path}/feasibility_Fratio_top10_2.csv\")\n",
        "feas3 = pd.read_csv(f\"{save_path}/feasibility_Fratio_top10_3.csv\")\n",
        "feas4 = pd.read_csv(f\"{save_path}/feasibility_Fratio_top10_4.csv\")\n",
        "feas5 = pd.read_csv(f\"{save_path}/feasibility_Fratio_top10_5.csv\")\n",
        "\n",
        "feas_list = [feas1, feas2, feas3, feas4, feas5]\n",
        "\n",
        "feas_summary = summarize_runs_by_vacancy(feas_list, index_col='vacancy_num', float_decimals=3)\n",
        "feas_summary[\"vacancy_num_int\"] = feas_summary[\"vacancy_num\"].str.extract(r\"(\\d+)\").astype(int)\n",
        "feas_summary = feas_summary.sort_values(by=\"vacancy_num_int\").drop(columns=[\"vacancy_num_int\"])\n",
        "\n",
        "# Ï†ÄÏû•\n",
        "feas_summary\n"
      ],
      "metadata": {
        "id": "-65iI1rC9LI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tre1 = pd.read_csv(f\"{save_path}/trendfit_1.csv\")\n",
        "tre2 = pd.read_csv(f\"{save_path}/trendfit_2.csv\")\n",
        "tre3 = pd.read_csv(f\"{save_path}/trendfit_3.csv\")\n",
        "tre4 = pd.read_csv(f\"{save_path}/trendfit_4.csv\")\n",
        "tre5 = pd.read_csv(f\"{save_path}/trendfit_5.csv\")\n",
        "\n",
        "tre_list = [tre1, tre2, tre3, tre4, tre5]\n",
        "\n",
        "tre_summary = summarize_runs_by_vacancy(tre_list, index_col='vacancy_num', float_decimals=3)\n",
        "tre_summary[\"vacancy_num_int\"] = tre_summary[\"vacancy_num\"].str.extract(r\"(\\d+)\").astype(int)\n",
        "tre_summary = tre_summary.sort_values(by=\"vacancy_num_int\").drop(columns=[\"vacancy_num_int\"])\n",
        "\n",
        "# Ï†ÄÏû•\n",
        "tre_summary"
      ],
      "metadata": {
        "id": "CV0hTuiE9LOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical test"
      ],
      "metadata": {
        "id": "SM-0DKUm3xDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "novelty - GPT, Claude, Llama"
      ],
      "metadata": {
        "id": "7m8p76ne4W6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import wilcoxon, friedmanchisquare\n",
        "\n",
        "# nov1~nov5\n",
        "nov_list = [nov1, nov2, nov3, nov4, nov5]\n",
        "\n",
        "# vacancy arrange + long-format\n",
        "records = []\n",
        "\n",
        "for run_id, df in enumerate(nov_list, start=1):\n",
        "    for _, row in df.iterrows():\n",
        "        vac = row['vacancy_num']\n",
        "\n",
        "        records.append({\n",
        "            'run': run_id,\n",
        "            'vacancy': vac,\n",
        "            'gpt_tech':   row['gpt_tech_lof'],\n",
        "            'gpt_both':   row['gpt_both_lof'],\n",
        "            'claude_tech': row['claude_tech_lof'],\n",
        "            'claude_both': row['claude_both_lof'],\n",
        "            'llama_tech':  row['llama_tech_lof'],\n",
        "            'llama_both':  row['llama_both_lof'],\n",
        "        })\n",
        "\n",
        "df_long = pd.DataFrame(records)\n",
        "df_long = df_long.sort_values(['run', 'vacancy']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "yii3ISGq30Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) GPT\n",
        "stat_gpt, p_gpt = wilcoxon(\n",
        "    df_long['gpt_tech'], df_long['gpt_both'], zero_method='wilcox'\n",
        ")\n",
        "\n",
        "# 2) Claude\n",
        "stat_claude, p_claude = wilcoxon(\n",
        "    df_long['claude_tech'], df_long['claude_both'], zero_method='wilcox'\n",
        ")\n",
        "\n",
        "# 3) Llama\n",
        "stat_llama, p_llama = wilcoxon(\n",
        "    df_long['llama_tech'], df_long['llama_both'], zero_method='wilcox'\n",
        ")\n",
        "\n",
        "print(\"Wilcoxon (tech vs both)\")\n",
        "print(f\"GPT    p={p_gpt:.4f}\")\n",
        "print(f\"Claude p={p_claude:.4f}\")\n",
        "print(f\"Llama  p={p_llama:.4f}\")\n",
        "\n",
        "#Wilcoxon (tech vs both)\n",
        "#GPT    p=0.4911\n",
        "#Claude p=0.0130\n",
        "#Llama  p=0.0327"
      ],
      "metadata": {
        "id": "2PXdwIQ530Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Friedman test: Î™®Îç∏ 3Í∞ú(gpt, claude, llama), both Î∞©Ïãù\n",
        "stat_fr, p_fr = friedmanchisquare(\n",
        "    df_long['gpt_both'],\n",
        "    df_long['claude_both'],\n",
        "    df_long['llama_both']\n",
        ")\n",
        "\n",
        "print(\"Friedman test (three models, both only)\")\n",
        "print(f\"stat={stat_fr:.4f}, p={p_fr:.4f}\")\n",
        "\n",
        "#Friedman test (three models, both only)\n",
        "#stat=9.3143, p=0.0095"
      ],
      "metadata": {
        "id": "74e_z6MO30fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "feasibility - GPT, Claude, Llama"
      ],
      "metadata": {
        "id": "NTWNeyjv4RtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import friedmanchisquare, wilcoxon\n",
        "\n",
        "# 1) 5Í∞ú run align + F_ratio Ïª¨ÎüºÎßå Ï∂îÏ∂ú\n",
        "cols_target = [\"gpt_F_ratio\", \"claude_F_ratio\", \"llama_F_ratio\"]\n",
        "\n",
        "aligned = []\n",
        "for df in feas_list:\n",
        "    df2 = df.copy()\n",
        "    df2 = df2.set_index(\"vacancy_num\").sort_index()\n",
        "    aligned.append(df2[cols_target])\n",
        "\n",
        "# 2) index / columns ÎèôÏùºÌïúÏßÄ Ï≤¥ÌÅ¨ (ÏïàÏ†ÑÏö©)\n",
        "base_index = aligned[0].index\n",
        "base_cols = aligned[0].columns\n",
        "for k, df in enumerate(aligned[1:], start=2):\n",
        "    if not df.index.equals(base_index):\n",
        "        raise ValueError(f\"{k}Î≤àÏß∏ DataFrameÏùò vacancy ÏàúÏÑú/Íµ¨ÏÑ±Ïù¥ Îã§Î¶ÖÎãàÎã§.\")\n",
        "    if not df.columns.equals(base_cols):\n",
        "        raise ValueError(f\"{k}Î≤àÏß∏ DataFrameÏùò columns Íµ¨ÏÑ±Ïù¥ Îã§Î¶ÖÎãàÎã§.\")\n",
        "\n",
        "# 3) (n_vacancy, n_cols, n_runs) ÌòïÌÉúÎ°ú stack\n",
        "stack = np.stack([df.values for df in aligned], axis=2)   # axis=2: run Ï∞®Ïõê\n",
        "means = stack.mean(axis=2)  # run 5ÌöåÏùò ÌèâÍ∑†\n",
        "\n",
        "# 4) ÌÜµÍ≥ÑÏö© DF (vacancyÎ≥Ñ, Î™®Îç∏Î≥Ñ ÌèâÍ∑† F_ratio)\n",
        "feas_means = pd.DataFrame(means, index=base_index, columns=cols_target).reset_index()\n",
        "\n",
        "# Friedman test: Í∞ôÏùÄ vacancyÏóêÏÑú gpt / claude / llama ÎπÑÍµê\n",
        "stat, p = friedmanchisquare(\n",
        "    feas_means[\"gpt_F_ratio\"],\n",
        "    feas_means[\"claude_F_ratio\"],\n",
        "    feas_means[\"llama_F_ratio\"],\n",
        ")\n",
        "\n",
        "print(\"Friedman chi-square =\", stat, \"p-value =\", p)\n",
        "\n",
        "\n",
        "#Friedman chi-square = 8.857142857142847 p-value = 0.011931522535756207"
      ],
      "metadata": {
        "id": "th06os0C30l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "trend-fit - GPT, Claude, Llama"
      ],
      "metadata": {
        "id": "-6vhU9FX4po4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tre_list = [tre1, tre2, tre3, tre4, tre5]\n",
        "\n",
        "trend_means = compute_means_from_runs(tre_list, index_col=\"vacancy_num\")\n",
        "\n",
        "# vacancy_num Ï†ïÎ†¨ (ÏõêÌïòÎ©¥)\n",
        "trend_means[\"vacancy_num_int\"] = trend_means[\"vacancy_num\"].str.extract(r\"(\\d+)\").astype(int)\n",
        "trend_means = trend_means.sort_values(\"vacancy_num_int\").drop(columns=[\"vacancy_num_int\"])\n"
      ],
      "metadata": {
        "id": "H7vByiVE4jTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import wilcoxon\n",
        "\n",
        "for model in [\"gpt\", \"claude\", \"llama\"]:\n",
        "    baseline_col = f\"{model}_tech_trend\"\n",
        "    ours_col     = f\"{model}_both_trend\"\n",
        "\n",
        "    stat, p = wilcoxon(\n",
        "        trend_means[baseline_col],\n",
        "        trend_means[ours_col],\n",
        "        alternative=\"two-sided\"\n",
        "    )\n",
        "    print(f\"{model}: Wilcoxon stat={stat}, p-value={p}\")\n",
        "\n",
        "#gpt: Wilcoxon stat=11.0, p-value=0.6875\n",
        "#claude: Wilcoxon stat=7.0, p-value=0.296875\n",
        "#llama: Wilcoxon stat=2.0, p-value=0.046875"
      ],
      "metadata": {
        "id": "pHLw55qb4zps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import friedmanchisquare\n",
        "\n",
        "cols_ours = [\"gpt_both_trend\", \"claude_both_trend\", \"llama_both_trend\"]\n",
        "\n",
        "# vacancyÎ≥Ñ Í∞íÎì§ÏùÑ ÌñâÏúºÎ°ú ÎÜìÍ≥†, Í∞Å Î™®Îç∏ÏùÑ Ïó¥Î°ú Î≥¥Îäî Íµ¨Ï°∞\n",
        "data_for_friedman = [trend_means[col].values for col in cols_ours]\n",
        "\n",
        "friedman_result = friedmanchisquare(*data_for_friedman)\n",
        "print(f\"Friedman chi-square = {friedman_result.statistic}, p-value = {friedman_result.pvalue}\")\n",
        "\n",
        "#Friedman chi-square = 12.285714285714278, p-value = 0.002148775480909733"
      ],
      "metadata": {
        "id": "w5tMtekH4m3U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}