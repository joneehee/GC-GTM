{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Keyword extraction by self-attention method"
      ],
      "metadata": {
        "id": "EBQ2P6WDDkAx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TASSMjc7DXmz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train2 = pd.read_csv('train_data.csv')\n",
        "train2.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK\n",
        "!pip install nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def is_noun(word):\n",
        "    # noun\n",
        "    pos = pos_tag([word])[0][1]\n",
        "    return pos.startswith('NN')\n",
        "\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Q-ZW3ZmoDiny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# attention\n",
        "def get_token_attention(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "    attn = outputs.attentions[-1][0].mean(0)[0].cpu().numpy()  # CLS\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    return tokens, attn\n",
        "\n",
        "# subword\n",
        "def merge_subwords(tokens, scores):\n",
        "    words, word_scores = [], []\n",
        "    buffer, buffer_scores = \"\", []\n",
        "\n",
        "    for tok, score in zip(tokens, scores):\n",
        "        if tok in ['[CLS]', '[SEP]']:\n",
        "            continue\n",
        "        if tok.startswith('##'):\n",
        "            buffer += tok[2:]\n",
        "            buffer_scores.append(score)\n",
        "        else:\n",
        "            if buffer:\n",
        "                words.append(buffer)\n",
        "                word_scores.append(np.mean(buffer_scores))\n",
        "            buffer = tok\n",
        "            buffer_scores = [score]\n",
        "    if buffer:\n",
        "        words.append(buffer)\n",
        "        word_scores.append(np.mean(buffer_scores))\n",
        "\n",
        "    return words, word_scores\n",
        "\n",
        "def average_duplicate_keywords(word_score_list):\n",
        "    score_dict = defaultdict(list)\n",
        "    for word, score in word_score_list:\n",
        "        score_dict[word].append(score)\n",
        "\n",
        "    averaged = [(word, float(np.mean(scores))) for word, scores in score_dict.items()]\n",
        "\n",
        "    return sorted(averaged, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# all_word_scores_arr: all word-level attention score arr\n",
        "scores = all_word_scores_arr\n",
        "\n",
        "# 5%, 10%, 15%, 20% cutoff\n",
        "thr_5  = np.percentile(scores, 5)\n",
        "thr_10 = np.percentile(scores, 10)\n",
        "thr_15 = np.percentile(scores, 15)\n",
        "thr_20 = np.percentile(scores, 20)\n",
        "\n",
        "print(\"5% threshold :\", thr_5)\n",
        "print(\"10% threshold:\", thr_10)\n",
        "print(\"15% threshold:\", thr_15)\n",
        "print(\"20% threshold:\", thr_20)\n",
        "\n",
        "def extract_keywords_for_threshold(words, scores, threshold):\n",
        "    pairs = [(w, s) for w, s in zip(words, scores)\n",
        "             if is_noun(w) and s >= threshold]\n",
        "\n",
        "    averaged = average_duplicate_keywords(pairs)\n",
        "\n",
        "    keywords = [w for w, _ in averaged]\n",
        "    return keywords\n",
        "\n",
        "rows = []\n",
        "\n",
        "# id grouping\n",
        "for doc_id, group in attn_detail_df.groupby('id'):\n",
        "    words  = group['word'].tolist()\n",
        "    scores = group['score'].tolist()\n",
        "\n",
        "    # abstract\n",
        "    abstract = train2.loc[train2['id'] == doc_id, 'text'].iloc[0]\n",
        "\n",
        "    # threshold extract\n",
        "    kw_5  = extract_keywords_for_threshold(words, scores, thr_5)\n",
        "    kw_10 = extract_keywords_for_threshold(words, scores, thr_10)\n",
        "    kw_15 = extract_keywords_for_threshold(words, scores, thr_15)\n",
        "    kw_20 = extract_keywords_for_threshold(words, scores, thr_20)\n",
        "\n",
        "    rows.append({\n",
        "        \"id\": doc_id,\n",
        "        \"abstract\": abstract,\n",
        "        \"kw_5\":  \", \".join(kw_5),\n",
        "        \"kw_10\": \", \".join(kw_10),\n",
        "        \"kw_15\": \", \".join(kw_15),\n",
        "        \"kw_20\": \", \".join(kw_20)\n",
        "    })\n",
        "\n",
        "result_df = pd.DataFrame(rows)\n",
        "result_df.to_csv('attn_5_10_15_20.csv', index=False)\n",
        "\n",
        "#5% threshold : 0.0009138039429672062\n",
        "#10% threshold: 0.001269497605971992\n",
        "#15% threshold: 0.0015857178019359707\n",
        "#20% threshold: 0.0018903562799096106"
      ],
      "metadata": {
        "id": "NpF9vL55Diqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_results = []\n",
        "for _, row in tqdm(train2.iterrows(), total=len(train2)):\n",
        "    doc_id = row['id']\n",
        "    text = row['text']\n",
        "\n",
        "    tokens, attn_scores = get_token_attention(text)\n",
        "    merged_words, merged_scores = merge_subwords(tokens, attn_scores)\n",
        "\n",
        "    scored = list(zip(merged_words, merged_scores))\n",
        "\n",
        "    # 명사만 필터링\n",
        "    noun_filtered = [(w, s) for w, s in scored if is_noun(w) and s >= 0.001269497605971992]\n",
        "\n",
        "    # 중복 단어 평균\n",
        "    averaged = average_duplicate_keywords(noun_filtered)\n",
        "\n",
        "    attn_results.append({\n",
        "        \"id\": doc_id,\n",
        "        \"keywords\": [w for w, _ in sorted(averaged, key=lambda x: -x[1])]\n",
        "    })\n",
        "attn_df = pd.DataFrame(attn_results)\n",
        "attn_df.head()\n",
        "attn_df.to_csv('attn_df.csv', index=False)"
      ],
      "metadata": {
        "id": "AByisohQDis0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract salient keyword"
      ],
      "metadata": {
        "id": "DT0AYQsSGpDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_df = pd.read_csv('attn_df.csv')\n",
        "\n",
        "# train(id, source) concat\n",
        "attn_df_merged = attn_df.merge(train2[['id', 'source']], on='id', how='left')"
      ],
      "metadata": {
        "id": "urCRNqj8DjrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "# ----------------------------\n",
        "# NLTK\n",
        "# ----------------------------\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# ----------------------------\n",
        "# base setting\n",
        "# ----------------------------\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# ----------------------------\n",
        "# POS mapping\n",
        "# ----------------------------\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# ----------------------------\n",
        "# keyword preprocessing\n",
        "# ----------------------------\n",
        "def clean_keywords_pos(keywords_list):\n",
        "    processed = []\n",
        "    for kw in keywords_list:\n",
        "        if not kw:\n",
        "            continue\n",
        "        kw_clean = re.sub(r'[^a-zA-Z]', '', kw.strip()).lower()\n",
        "        if len(kw_clean) <= 2:\n",
        "            continue\n",
        "        pos = pos_tag([kw_clean])[0][1]\n",
        "        wn_pos = get_wordnet_pos(pos)\n",
        "        if wn_pos == wordnet.VERB:\n",
        "            continue\n",
        "\n",
        "        lemma = lemmatizer.lemmatize(kw_clean, pos=wn_pos)\n",
        "        processed.append(lemma)\n",
        "    return processed\n",
        "\n",
        "# ----------------------------\n",
        "# list processing\n",
        "# ----------------------------\n",
        "def preprocess_keywords(row):\n",
        "    if isinstance(row, str):\n",
        "        keyword_list = [k.strip() for k in row.split(',')]\n",
        "    elif isinstance(row, list):\n",
        "        keyword_list = row\n",
        "    else:\n",
        "        return []\n",
        "    return clean_keywords_pos(keyword_list)\n",
        "\n",
        "\n",
        "attn_df_merged['cleaned_keywords'] = attn_df_merged['keywords'].apply(preprocess_keywords)\n",
        "\n",
        "# count by source\n",
        "patent_df = attn_df_merged[attn_df_merged['source'] == 'patent']\n",
        "startup_df = attn_df_merged[attn_df_merged['source'] == 'startup']\n",
        "\n",
        "patent_keywords_flat = [kw for kws in patent_df['cleaned_keywords'] for kw in kws]\n",
        "startup_keywords_flat = [kw for kws in startup_df['cleaned_keywords'] for kw in kws]\n",
        "\n",
        "patent_counts = Counter(patent_keywords_flat)\n",
        "startup_counts = Counter(startup_keywords_flat)\n",
        "\n",
        "# top keywords filtering\n",
        "def top_percent_keywords(counter_obj, percent=0.10):\n",
        "    sorted_kws = sorted(counter_obj.items(), key=lambda x: x[1], reverse=True)\n",
        "    cutoff = max(1, int(len(sorted_kws) * percent))\n",
        "    return set([kw for kw, _ in sorted_kws[:cutoff]])\n",
        "\n",
        "patent_top_keywords = top_percent_keywords(patent_counts, 0.10)\n",
        "startup_top_keywords = top_percent_keywords(startup_counts, 0.10)\n",
        "\n",
        "combined_top_keywords = patent_top_keywords.union(startup_top_keywords)\n",
        "\n",
        "# ----------------------------\n",
        "# final keyword\n",
        "# ----------------------------\n",
        "def filter_to_top_keywords(keywords, top_keywords_set):\n",
        "    return [kw for kw in keywords if kw in top_keywords_set]\n",
        "\n",
        "attn_df_merged['final_keywords'] = attn_df_merged['cleaned_keywords'].apply(\n",
        "    lambda kws: filter_to_top_keywords(kws, combined_top_keywords)\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# doc-term matrix\n",
        "# ----------------------------\n",
        "doc_term_matrix = pd.DataFrame(0, index=attn_df_merged['id'], columns=sorted(combined_top_keywords))\n",
        "\n",
        "for idx, keywords in zip(attn_df_merged['id'], attn_df_merged['final_keywords']):\n",
        "    for kw in keywords:\n",
        "        doc_term_matrix.loc[idx, kw] = 1"
      ],
      "metadata": {
        "id": "z-YXmzBjHKCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tf-idf vectorize"
      ],
      "metadata": {
        "id": "2_KEJn4sIkAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 step stopwords processing\n",
        "- 1st level : verb, stopwords\n",
        "- 2nd level : general business and concept term\n",
        "- 3rd level : general tech, attribute, business term"
      ],
      "metadata": {
        "id": "8_8FoUl-9a4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_keywords = [word for word in combined_top_keywords if word.lower() not in stop_words and word]\n",
        "\n",
        "first_level_stopwords = [\n",
        "    'use', 'allow', 'receive', 'contain', 'implement', 'combine', 'leverage', 'describe', 'learn', 'classify', 'simulate', 'self', 'mean', 'clean', 'aim',\n",
        "    'quality', 'well', 'world', 'call', 'idea', 'post', 'form', 'back', 'subject', 'cover', 'allow', 'benefit', 'change', 'play', 'set', 'start',\n",
        "    'com', 'future', 'claim', 'time', 'order', 'word', 'answer', 'multiple', 'entry', 'response', 'function', 'contact', 'instance', 'contain', 'cost'\n",
        "]\n",
        "\n",
        "revised_second_level_stopwords = [\n",
        "    # general business term\n",
        "    'channel', 'line', 'message', 'query', 'brand', 'creator', 'sector', 'account', 'client', 'growth', 'advisory',\n",
        "    'industry', 'employee', 'insight', 'market', 'sale', 'marketing', 'business', 'manager', 'enterprise', 'partner', 'partnership',\n",
        "    'community', 'management', 'policy', 'organisation', 'workforce', 'analytics', 'performance', 'confidence', 'scale', 'score', 'observation',\n",
        "    'determination', 'cluster', 'frequency', 'deployment', 'procedure', 'production', 'google', 'workflow',\n",
        "\n",
        "    # general concept\n",
        "    'technique', 'development', 'behavior', 'operation', 'implementation', 'objective', 'state', 'base', 'ground', 'profile', 'anchor',\n",
        "    'formation', 'arrangement', 'shape', 'measure', 'section', 'aspect', 'factor', 'creation', 'approach', 'characteristic', 'life',\n",
        "]\n",
        "\n",
        "final_stopwords = [\n",
        "    # general tech term\n",
        "    'system', 'component', 'implementation', 'technology', 'framework', 'structure', 'base', 'architecture',\n",
        "    'feature', 'configuration', 'arrangement', 'mechanism', 'invention',\n",
        "\n",
        "    # general attribute\n",
        "    'pressure', 'heat', 'hollow', 'transparent', 'environment', 'fabric', 'color', 'position', 'distance', 'focus', 'quality', 'characteristic',\n",
        "    'factor', 'aspect', 'attribute', 'part', 'portion', 'section', 'fluid', 'start',\n",
        "\n",
        "    # business term\n",
        "    'retailer', 'agency', 'government', 'participant', 'lift', 'food', 'operator', 'expert', 'building', 'turf', 'crop', 'pair', 'weather',\n",
        "    'tree', 'branch', 'artist', 'fish', 'fixture', 'kit', 'gesture', 'shop', 'fund', 'investment', 'culture', 'leather', 'article', 'startup',\n",
        "    'shell', 'insurance', 'decision', 'faster', 'market', 'business', 'industry', 'client', 'customer', 'campaign', 'advisory', 'employee', 'workforce', 'organisation', 'community', 'policy', 'benefit',\n",
        "]\n",
        "\n",
        "# ----------------------------\n",
        "\n",
        "def lemmatize_stopwords(stopword_list):\n",
        "    return set(lemmatizer.lemmatize(w.lower(), wordnet.NOUN) for w in stopword_list)\n",
        "\n",
        "first_level_stopwords_lemma = lemmatize_stopwords(first_level_stopwords)\n",
        "revised_second_level_stopwords_lemma = lemmatize_stopwords(revised_second_level_stopwords)\n",
        "final_stopwords_lemma = lemmatize_stopwords(final_stopwords)\n",
        "custom_stopwords_lemma = lemmatize_stopwords(list(custom_stopwords))\n",
        "\n",
        "\n",
        "def clean_keywords_final(keywords_list):\n",
        "\n",
        "    cleaned_list = [kw.lower() for kw in keywords_list if kw]\n",
        "\n",
        "    after_first = [kw for kw in cleaned_list if kw not in first_level_stopwords_lemma]\n",
        "\n",
        "    after_second = [kw for kw in after_first if kw not in revised_second_level_stopwords_lemma]\n",
        "\n",
        "    final_cleaned = [kw for kw in after_second if kw not in final_stopwords_lemma and kw not in custom_stopwords_lemma]\n",
        "\n",
        "    return final_cleaned\n",
        "\n",
        "# ----------------------------\n",
        "\n",
        "final_cleaned_keywords = clean_keywords_final(list(combined_top_keywords))\n",
        "\n",
        "keywords_to_exclude = ['event', 'analysis', 'pipeline', 'startup', 'material', 'water', 'surface', 'composition', 'weight']\n",
        "final_combined_keywords = [kw for kw in final_cleaned_keywords if kw not in keywords_to_exclude]\n",
        "\n",
        "# ----------------------------\n",
        "\n",
        "patent_keyword_freq = dict(sorted_patent_keywords)\n",
        "startup_keyword_freq = dict(sorted_startup_keywords)\n",
        "\n",
        "final_combined_freq = {}\n",
        "for kw in final_combined_keywords:\n",
        "    patent_freq = patent_keyword_freq.get(kw, 0)\n",
        "    startup_freq = startup_keyword_freq.get(kw, 0)\n",
        "    final_combined_freq[kw] = patent_freq + startup_freq"
      ],
      "metadata": {
        "id": "fAOhEkOq72dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def map_to_final_keywords(keywords, final_vocab_set):\n",
        "    lemmatized = [lemmatizer.lemmatize(kw.lower(), wordnet.NOUN) for kw in keywords]\n",
        "    return ' '.join([kw for kw in lemmatized if kw in final_vocab_set])\n",
        "\n",
        "final_vocab_set = set(final_combined_keywords)\n",
        "\n",
        "attn_df_merged['tfidf_text'] = attn_df_merged['cleaned_keywords'].apply(\n",
        "    lambda kws: map_to_final_keywords(kws, final_vocab_set)\n",
        ")\n",
        "\n",
        "# TF-IDF vectorize\n",
        "vectorizer = TfidfVectorizer(vocabulary=final_combined_keywords)\n",
        "\n",
        "# TF-IDF matrix\n",
        "tfidf_matrix = vectorizer.fit_transform(attn_df_merged['tfidf_text'])\n",
        "\n",
        "# to DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(),\n",
        "                        columns=vectorizer.get_feature_names_out(),\n",
        "                        index=attn_df_merged['id'])\n",
        "tfidf_df"
      ],
      "metadata": {
        "id": "kooKOWHiIpth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_df.to_csv('tfidf_df.csv', index=False)\n",
        "\n",
        "# 키워드별 평균 TF-IDF 값 계산\n",
        "mean_tfidf_per_keyword = tfidf_df.mean(axis=0).sort_values(ascending=False)\n",
        "\n",
        "# 결과 확인 (상위 20개 키워드)\n",
        "print(mean_tfidf_per_keyword.head(20))\n",
        "\n",
        "#device         0.103101\n",
        "#processing     0.067296\n",
        "#sensor         0.057862\n",
        "#processor      0.056383\n",
        "#image          0.044627 ..."
      ],
      "metadata": {
        "id": "8wTo3aNLJaE0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}